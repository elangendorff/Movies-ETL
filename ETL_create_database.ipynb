{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "from config import db_password\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Add the clean movie function that takes in the argument, \"movie\".\n",
    "def clean_movie(movie):\n",
    "    movie = dict(movie)  # local copy of the input parameter dictionary\n",
    "    \n",
    "    # List of languages\n",
    "    language_columns = [\n",
    "        'Also known as',\n",
    "        'Arabic',\n",
    "        'Bopomofo',\n",
    "        'Cantonese',\n",
    "        'Chinese',\n",
    "        'French',\n",
    "        'Gwoyeu Romatzyh',\n",
    "        'Hangul',\n",
    "        'Hanyu Pinyin',\n",
    "        'Hebrew',\n",
    "        'Hepburn',\n",
    "        'Hokkien POJ',\n",
    "        'IPA',\n",
    "        'Japanese',\n",
    "        'Jyutping',\n",
    "        'Literally',\n",
    "        'Mandarin',\n",
    "        'McCune–Reischauer',\n",
    "        'Original title',\n",
    "        'Polish',\n",
    "        'Revised Romanization',\n",
    "        'Romanized',\n",
    "        'Russian',\n",
    "        'Simplified',\n",
    "        'Simplified Chinese',\n",
    "        'Traditional',\n",
    "        'Traditional Chinese',\n",
    "        'Wade–Giles',\n",
    "        'Yale Romanization',\n",
    "        'Yiddish'\n",
    "    ]\n",
    "    \n",
    "    alt_titles = {}\n",
    "    for key in language_columns:\n",
    "        if key in movie:\n",
    "            alt_titles[key] = movie.pop(key)\n",
    "    if 0 < len(alt_titles):\n",
    "        movie['alt_titles'] = alt_titles\n",
    "    \n",
    "    # list of columns to be merged\n",
    "    merge_columns = [\n",
    "        ['Adaptation by', 'Writer(s)'],\n",
    "        ['Country of origin', 'Country'],\n",
    "        ['Directed by', 'Director'],\n",
    "        ['Distributed by', 'Distributor'],\n",
    "        ['Edited by', 'Editor(s)'],\n",
    "        ['Length', 'Running time'],\n",
    "        ['Music by', 'Composer(s)'],\n",
    "        ['Original language(s)', 'Language'],\n",
    "        ['Original release', 'Release date'],\n",
    "        ['Produced by', 'Producer(s)'],\n",
    "        ['Producer', 'Producer(s)'],\n",
    "        ['Productioncompanies ', 'Production company(s)'],\n",
    "        ['Productioncompany ', 'Production company(s)'],\n",
    "        ['Release Date', 'Release date'],\n",
    "        ['Released', 'Release Date'],\n",
    "        ['Screen story by', 'Writer(s)'],\n",
    "        ['Screenplay by', 'Writer(s)'],\n",
    "        ['Story by', 'Writer(s)'],\n",
    "        ['Theme music composer', 'Composer(s)'],\n",
    "        ['Written by', 'Writer(s)']\n",
    "    ]\n",
    "    \n",
    "    for col in merge_columns:\n",
    "        if col[0] in movie:                     # If the first column name exists…\n",
    "            movie[col[1]] = movie.pop(col[0])   # …rename that column to the second name\n",
    "                                                # (overwriting the second column, if it exists)\n",
    "    \n",
    "    return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Add the function that takes in three arguments;\n",
    "# Wikipedia data, Kaggle metadata, and MovieLens rating data (from Kaggle)\n",
    "\n",
    "def extract_transform_load(wiki_file, kaggle_file, ratings_file):\n",
    "    # Read in the kaggle metadata and MovieLens ratings CSV files as Pandas DataFrames.\n",
    "    df_kaggle = pd.read_csv(kaggle_file, low_memory=False)\n",
    "    df_ratings = pd.read_csv(ratings_file)\n",
    "    \n",
    "    # Open and read the Wikipedia data JSON file.\n",
    "    with open(wiki_file, mode='r') as file:\n",
    "        wiki_movies_raw = json.load(file)\n",
    "    \n",
    "    # Write a list comprehension to filter out TV shows.\n",
    "    wiki_movies = [movie for movie in wiki_movies_raw\n",
    "        if True\n",
    "        and ('Directed by' in movie or 'Director' in movie)\n",
    "        and 'imdb_link' in movie\n",
    "        and 'No. of episodes' not in movie\n",
    "    ]\n",
    "    \n",
    "    # Write a list comprehension to iterate through the cleaned wiki movies list\n",
    "    # and call the clean_movie function on each movie.\n",
    "    clean_movies = [clean_movie(movie) for movie in wiki_movies]\n",
    "    \n",
    "    # Read in the cleaned movies list from Step 4 as a DataFrame.\n",
    "    df_wiki = pd.DataFrame(clean_movies)\n",
    "\n",
    "    # Write a try-except block to catch errors while extracting the IMDb ID using a regular expression string and\n",
    "    #  dropping any imdb_id duplicates. If there is an error, capture and print the exception.\n",
    "    try:\n",
    "        df_wiki['imdb_id'] = (\n",
    "            df_wiki['imdb_link']\n",
    "#             .str.strip()  # Make sure there is no extraneous whitespace on either end\n",
    "#             .str.extract(r'^https?:\\/\\/(?:www\\.)?imdb\\.com\\/title\\/(tt\\d{7})\\/?$')\n",
    "            .str.extract(r'(tt\\d{7})')\n",
    "        )\n",
    "        \n",
    "        df_wiki.drop_duplicates(subset='imdb_id', inplace=True)\n",
    "#         raise ExpectedError()  # for testing\n",
    "    except BaseException as err:\n",
    "        print(f\"Unexpected {err}, {type(err)}\")  # From https://docs.python.org/3/tutorial/errors.html\n",
    "    \n",
    "#     ####\n",
    "#     print('df_wiki rows:',len(df_wiki),'(7033 in Module; different regex)')##############################################\n",
    "#     ####\n",
    "    \n",
    "    #  Write a list comprehension to keep the columns that don't have null values from the wiki_movies_df DataFrame.\n",
    "    df_wiki = df_wiki[[col for col in df_wiki.columns if df_wiki[col].isnull().sum() < 0.9*len(df_wiki)]]\n",
    "    \n",
    "    # Create a variable that will hold the non-null values from the “Box office” column.\n",
    "    box_office = df_wiki['Box office'].dropna()\n",
    "    \n",
    "    # Convert the box office data created in Step 8 to string values using the lambda and join functions.\n",
    "    box_office = box_office.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    \n",
    "    # Replace box office dollar ranges with just the upper value\n",
    "    box_office = box_office.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "    \n",
    "    # Write a regular expression to match the six elements of \"form_one\" of the box office data.\n",
    "    dollars_pattern_mil = r'\\$\\s*\\d+\\.?\\d*\\s*mill?i?on'\n",
    "    dollars_pattern_bil = r'\\$\\s*\\d+\\.?\\d*\\s*bill?i?on'\n",
    "    \n",
    "    # Write a regular expression to match the three elements of \"form_two\" of the box office data.\n",
    "    dollars_pattern_num = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)'\n",
    "    \n",
    "    dollars_pattern_combined = f'({dollars_pattern_mil}|{dollars_pattern_bil}|{dollars_pattern_num})'\n",
    "    \n",
    "    # Add the parse_dollars function.\n",
    "    def parse_dollars(s):\n",
    "        # if s is not a string, return NaN\n",
    "        if type(s) != str:\n",
    "            return np.nan\n",
    "\n",
    "        # if input is of the form $###.# million\n",
    "        if re.match(dollars_pattern_mil, s, flags=re.IGNORECASE):\n",
    "\n",
    "            # remove dollar sign and \" million\"\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)  # Why not keep only \\d and \\. characters instead of\n",
    "                                                # removing those specific ones?\n",
    "\n",
    "            # convert to float and multiply by a million\n",
    "            value = float(s) * 10**6\n",
    "\n",
    "            # return value\n",
    "            return value\n",
    "\n",
    "        # if input is of the form $###.# billion\n",
    "        elif re.match(dollars_pattern_bil, s, flags=re.IGNORECASE):\n",
    "\n",
    "            # remove dollar sign and \" billion\"\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)  # See question in the 'million' section, above\n",
    "\n",
    "            # convert to float and multiply by a billion\n",
    "            value = float(s) * 10**9\n",
    "\n",
    "            # return value\n",
    "            return value\n",
    "\n",
    "        # if input is of the form $###,###,###\n",
    "        elif re.match(dollars_pattern_num, s, flags=re.IGNORECASE):\n",
    "\n",
    "            # remove dollar sign and commas\n",
    "            s = re.sub('\\$|,','', s)  # Same question\n",
    "\n",
    "            # convert to float\n",
    "            value = float(s)\n",
    "\n",
    "            # return value\n",
    "            return value\n",
    "\n",
    "        # otherwise, return NaN\n",
    "        else:\n",
    "            return np.nan\n",
    "    \n",
    "    # Clean the box office column in the wiki_movies_df DataFrame.\n",
    "    df_wiki['box_office'] = box_office.str.extract(\n",
    "        dollars_pattern_combined,\n",
    "        flags=re.IGNORECASE\n",
    "    )[0].apply(parse_dollars)\n",
    "    \n",
    "    df_wiki.drop('Box office', axis=1, inplace=True)\n",
    "    \n",
    "    # Clean the budget column in the wiki_movies_df DataFrame.\n",
    "    df_wiki['budget'] = (\n",
    "        df_wiki['Budget']\n",
    "        .dropna()\n",
    "        .apply(lambda x: ' '.join(x) if type(x) == list else x) # Join list elements into space-separated strings\n",
    "        .str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)    # Replace dollar ranges with just the upper value\n",
    "        .str.replace(r'\\s*\\[\\d+\\]', '', regex=True)             # Remove citations (numbers within square brackets)\n",
    "        .str.extract(dollars_pattern_combined, flags=re.IGNORECASE)[0]\n",
    "        .apply(parse_dollars)\n",
    "    )\n",
    "    \n",
    "#     df_wiki.drop('Budget', axis=1, inplace=True)\n",
    "    \n",
    "    # Clean the release date column in the wiki_movies_df DataFrame.\n",
    "    date_pattern_ymd = r'\\d{4}[/-][01]?\\d[/-]][0123]?\\d'\n",
    "    date_pattern_mdy = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}'\n",
    "    date_pattern_dmy = r'\\d{1,2}\\s+(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{4}'\n",
    "    date_pattern_my = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{4}'\n",
    "    date_pattern_yo = r'^\\d{4}$|(?<=\\( )\\d{4}(?= \\))'\n",
    "\n",
    "    date_pattern_combined = f'({date_pattern_ymd}|{date_pattern_mdy}|{date_pattern_dmy}|{date_pattern_my}|{date_pattern_yo})'\n",
    "\n",
    "    df_wiki['release_date'] = pd.to_datetime(\n",
    "        df_wiki['Release date']\n",
    "        .dropna()\n",
    "        .apply(lambda x: ' '.join(x) if type(x) == list else x) # Join list elements into space-separated strings\n",
    "        .str.extract(date_pattern_combined, flags=re.IGNORECASE )[0],\n",
    "        infer_datetime_format=True\n",
    "    )\n",
    "    \n",
    "#     df_wiki.drop('Release date', axis=1, inplace=True)\n",
    "    \n",
    "    # Clean the running time column in the wiki_movies_df DataFrame.\n",
    "    run_time_pattern_hrs = r'(\\d+)\\s+h(?:(?:ou)?rs?)?(?:\\s(\\d+)\\s+m(?:in(?:utes)?)?)?'\n",
    "    run_time_pattern_min = r'(\\d+)\\s+m(?:in(?:utes)?)?'\n",
    "\n",
    "    run_time_pattern_combined = f'{run_time_pattern_hrs}|{run_time_pattern_min}'\n",
    "\n",
    "    df_wiki['running_time'] = (\n",
    "        df_wiki['Running time']\n",
    "        .dropna()\n",
    "        .apply(lambda x: ' '.join(x) if type(x) == list else x) # Join list elements into space-separated strings\n",
    "        .str.extract(run_time_pattern_combined)                 # Extract times: column 0 is hours; columns 1 & 2 are minutes\n",
    "        .apply(lambda col: pd.to_numeric(col, errors='coerce')) # Coerce empty strings (errors) into NaNs\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "        .apply(lambda row: 60*row[0]+row[1]+row[2], axis=1)     # Convert hours/minutes/minutes to minutes\n",
    "    )\n",
    "    \n",
    "    df_wiki.drop('Running time', axis=1, inplace=True)\n",
    "\n",
    "    # 2. Clean the Kaggle metadata.\n",
    "    df_kaggle = df_kaggle[df_kaggle['adult'] == 'False'].drop('adult', axis=1)  # Drop adult movies\n",
    "    df_kaggle['video'] = df_kaggle['video'].fillna(False)                       # Make the video column boolean\n",
    "                                                                                # (treat NaNs as False)\n",
    "    df_kaggle['budget'] = df_kaggle['budget'].astype(int)                       # Convert budget column to integer type\n",
    "    df_kaggle['id'] = pd.to_numeric(                                            # Convert id column to numeric type\n",
    "        df_kaggle['id'],\n",
    "        errors='raise'  # see https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html\n",
    "    )\n",
    "    df_kaggle['popularity'] = pd.to_numeric(df_kaggle['popularity'], errors='raise')\n",
    "    df_kaggle['release_date'] = pd.to_datetime(df_kaggle['release_date'])\n",
    "    \n",
    "    # 3. Merge the two DataFrames into the movies DataFrame.\n",
    "    df_movies = pd.merge(df_wiki, df_kaggle, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "    \n",
    "#     df_movies = df_movies.drop(\n",
    "#         df_movies[\n",
    "#             (df_movies['release_date_kaggle'] < '1965-01-01')\n",
    "#             & ('1996-01-01' < df_movies['release_date_wiki'])\n",
    "#         ].index\n",
    "#     ) # Bad merger: The Holiday (2006) merged with From Here to Eternity(1953)\n",
    "#     \n",
    "#     ####\n",
    "#     print(\n",
    "#         'df_movies rows:',\n",
    "#         len(df_movies),\n",
    "#         '(6051 in Module; removed bad merger: The Holiday (2006)/From Here to Eternity(1953))')##########################\n",
    "#     ####\n",
    "    \n",
    "    # 4. Drop unnecessary columns from the merged DataFrame.\n",
    "    df_movies.drop(columns=['title_wiki','release_date_wiki','Language','Production company(s)'], inplace=True)\n",
    "\n",
    "    # 5. Add in the function to fill in the missing Kaggle data.\n",
    "    def fill_missing_data(df, column_to_keep, column_to_drop):\n",
    "        df[column_to_keep] = df.apply(\n",
    "            lambda row: row[column_to_drop] if row[column_to_keep] == 0 else row[column_to_keep]\n",
    "            , axis=1)\n",
    "        df.drop(columns=column_to_drop, inplace=True)\n",
    "    \n",
    "    # 6. Call the function in Step 5 with the DataFrame and columns as the arguments.\n",
    "    fill_missing_data(df_movies, 'runtime', 'running_time')\n",
    "    fill_missing_data(df_movies, 'budget_kaggle', 'budget_wiki')\n",
    "    fill_missing_data(df_movies, 'box_office', 'revenue')\n",
    "    \n",
    "    # 7. Filter the movies DataFrame for specific columns.\n",
    "    df_movies = df_movies.loc[:, [\n",
    "        'imdb_id',\n",
    "        'id',\n",
    "        'title_kaggle',\n",
    "        'original_title',\n",
    "        'belongs_to_collection',\n",
    "        'tagline',\n",
    "        'overview',\n",
    "        'url',\n",
    "        'imdb_link',\n",
    "        'runtime',\n",
    "        'budget_kaggle',\n",
    "        'box_office',  # Differs from Module\n",
    "        'year',\n",
    "        'release_date_kaggle',\n",
    "        'popularity',\n",
    "        'vote_average',\n",
    "        'vote_count',\n",
    "        'genres',\n",
    "        'original_language',\n",
    "        'spoken_languages',\n",
    "        'Country',\n",
    "        'production_companies',\n",
    "        'production_countries',\n",
    "        'Distributor',\n",
    "        'Producer(s)',\n",
    "        'Director',\n",
    "        'Writer(s)',\n",
    "        'Based on',\n",
    "        'Composer(s)',\n",
    "        'Starring',\n",
    "        'Cinematography',\n",
    "        'Editor(s)'\n",
    "#         'Release date', # Skip\n",
    "#         'homepage',     # Skip\n",
    "#         'poster_path',  # Skip\n",
    "#         'status',       # Skip\n",
    "#         'video',        # Skip\n",
    "    ]]\n",
    "    \n",
    "    # 8. Rename the columns in the movies DataFrame.\n",
    "    df_movies.rename({\n",
    "        'id':'kaggle_id',\n",
    "        'title_kaggle':'title',\n",
    "        'url':'wikipedia_url',\n",
    "        'budget_kaggle':'budget',\n",
    "        'box_office':'revenue',\n",
    "        'release_date_kaggle':'release_date',\n",
    "        'Country':'country',\n",
    "        'Distributor':'distributor',\n",
    "        'Producer(s)':'producers',\n",
    "        'Director':'director',\n",
    "        'Starring':'starring',\n",
    "        'Cinematography':'cinematography',\n",
    "        'Editor(s)':'editors',\n",
    "        'Writer(s)':'writers',\n",
    "        'Composer(s)':'composers',\n",
    "        'Based on':'based_on'\n",
    "    }, axis='columns', inplace=True)\n",
    "    \n",
    "    # 9. Transform and merge the ratings DataFrame.\n",
    "    df_ratings['timestamp'] = pd.to_datetime(df_ratings['timestamp'], unit='s') # unit='s' is for seconds\n",
    "    \n",
    "    rating_counts = (\n",
    "        df_ratings.groupby(['movieId','rating'], as_index=False).count()\n",
    "        .rename({'userId':'count'}, axis=1)  # the userId and timestamp columns each now contain counts for the above\n",
    "        .pivot(index='movieId',columns='rating', values='count')\n",
    "    )\n",
    "    \n",
    "    rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]\n",
    "    \n",
    "    df_movies_w_ratings = pd.merge(\n",
    "        df_movies,              # The left table\n",
    "        rating_counts,          # The right table\n",
    "        how='left',             # Join type\n",
    "        left_on='kaggle_id',    # Column from the left table to use as the join key\n",
    "        right_index=True        # Use the right table's index column for the join key instead of using 'right_on='\n",
    "    )\n",
    "    \n",
    "    df_movies_w_ratings[rating_counts.columns] = df_movies_w_ratings[rating_counts.columns].fillna(0).astype(int)\n",
    "    \n",
    "#     # Row counts to check the results of export steps to follow\n",
    "#     # The :>10 parts of the f-strings mean to have the output right justified ten characters (digits) over.\n",
    "#     print(\n",
    "#         f\"Movies (w/ ratings): {str(len(df_movies)) + '(' + str(len(df_movies_w_ratings)) + ')':>10} rows\\n\"\n",
    "#         f\"Ratings:            {len(df_ratings):>10}  rows\\n\"\n",
    "#     )\n",
    "    \n",
    "    # create connection to PostgreSQL database\n",
    "    engine = create_engine(f\"postgresql://postgres:{db_password}@127.0.0.1:5432/movie_data\")\n",
    "    \n",
    "    # Upload df_movies as SQL table\n",
    "    df_movies.to_sql(name='movies', con=engine, if_exists='replace')\n",
    "    \n",
    "    # Export Ratings data\n",
    "    rows_imported = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for data in pd.read_csv(ratings_file, chunksize=1000000):\n",
    "        \n",
    "        # print out the range of rows that are being imported\n",
    "        print(f'Uploading rows {rows_imported} to {rows_imported + len(data)}... ', end='')\n",
    "        \n",
    "        data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "        \n",
    "        # increment the number of rows imported by the size of 'data'\n",
    "        rows_imported += len(data)\n",
    "        \n",
    "        # print that the rows have finished importing\n",
    "        print(f'done. {time.time() - start_time} total seconds elapsed.')\n",
    "    \n",
    "    print('Upload complete.')\n",
    "    \n",
    "#     return df_wiki, df_movies_w_ratings, df_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filepath directory and variables for the three files.\n",
    "filepath = './Resources'\n",
    "# The Wikipedia data\n",
    "wiki_file = f'{filepath}/wikipedia-movies.json'\n",
    "# The Kaggle metadata\n",
    "kaggle_file = f'{filepath}/movies_metadata.csv'\n",
    "# The MovieLens rating data.\n",
    "ratings_file = f'{filepath}/ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading rows 0 to 1000000... done. 38.77549386024475 total seconds elapsed.\n",
      "Uploading rows 1000000 to 2000000... done. 77.16441988945007 total seconds elapsed.\n",
      "Uploading rows 2000000 to 3000000... done. 114.51331233978271 total seconds elapsed.\n",
      "Uploading rows 3000000 to 4000000... done. 151.37267780303955 total seconds elapsed.\n",
      "Uploading rows 4000000 to 5000000... done. 188.2217025756836 total seconds elapsed.\n",
      "Uploading rows 5000000 to 6000000... done. 226.1458158493042 total seconds elapsed.\n",
      "Uploading rows 6000000 to 7000000... done. 265.24549198150635 total seconds elapsed.\n",
      "Uploading rows 7000000 to 8000000... done. 302.25529193878174 total seconds elapsed.\n",
      "Uploading rows 8000000 to 9000000... done. 340.3410987854004 total seconds elapsed.\n",
      "Uploading rows 9000000 to 10000000... done. 377.23037457466125 total seconds elapsed.\n",
      "Uploading rows 10000000 to 11000000... done. 414.3162200450897 total seconds elapsed.\n",
      "Uploading rows 11000000 to 12000000... done. 453.0658755302429 total seconds elapsed.\n",
      "Uploading rows 12000000 to 13000000... done. 490.2300012111664 total seconds elapsed.\n",
      "Uploading rows 13000000 to 14000000... done. 527.0256631374359 total seconds elapsed.\n",
      "Uploading rows 14000000 to 15000000... done. 565.4160041809082 total seconds elapsed.\n",
      "Uploading rows 15000000 to 16000000... done. 601.9384233951569 total seconds elapsed.\n",
      "Uploading rows 16000000 to 17000000... done. 638.7140493392944 total seconds elapsed.\n",
      "Uploading rows 17000000 to 18000000... done. 676.1034979820251 total seconds elapsed.\n",
      "Uploading rows 18000000 to 19000000... done. 713.3701555728912 total seconds elapsed.\n",
      "Uploading rows 19000000 to 20000000... done. 749.9566028118134 total seconds elapsed.\n",
      "Uploading rows 20000000 to 21000000... done. 786.560967206955 total seconds elapsed.\n",
      "Uploading rows 21000000 to 22000000... done. 823.7123336791992 total seconds elapsed.\n",
      "Uploading rows 22000000 to 23000000... done. 861.5522973537445 total seconds elapsed.\n",
      "Uploading rows 23000000 to 24000000... done. 897.8013832569122 total seconds elapsed.\n",
      "Uploading rows 24000000 to 25000000... done. 935.3947536945343 total seconds elapsed.\n",
      "Uploading rows 25000000 to 26000000... done. 972.8319618701935 total seconds elapsed.\n",
      "Uploading rows 26000000 to 26024289... done. 973.7834858894348 total seconds elapsed.\n",
      "Upload complete.\n"
     ]
    }
   ],
   "source": [
    "extract_transform_load(wiki_file, kaggle_file, ratings_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
